{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime as dt\n",
    "\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.tools import add_constant\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import tree\n",
    "from sklearn import svm\n",
    "from sklearn import neighbors\n",
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "historical_data = pd.read_csv('historical_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_id_unique = historical_data[\"store_id\"].unique().tolist()\n",
    "store_id_and_category = {store_id: historical_data[historical_data.store_id == store_id].store_primary_category.mode() \n",
    "                         for store_id in store_id_unique}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill(store_id):\n",
    "    try:\n",
    "        return store_id_and_category[store_id].values[0]\n",
    "    except:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill null values\n",
    "historical_data[\"clean_store_primary_category\"] = historical_data.store_id.apply(fill)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "## note that the number of non-null store_id values is less than the number of non-null \n",
    "# store_primary category values. We can probably fill in most of the missing values via inference\n",
    "\n",
    "## there are also stores with multiple store_primary_category values. Such stores might be 'ghost kitchens',\n",
    "# and need to be backfilled appropriately\n",
    "# TODO #1 write function to downfill NaN store_primary_category values\n",
    "\n",
    "def fillNulls_store_category(historical_data:pd.DataFrame) -> pd.DataFrame:\n",
    "    store_id_list = historical_data['store_id'].unique().tolist()\n",
    "    store_id_cat = {store_id: historical_data[historical_data.store_id == store_id].store_primary_category.mode() for store_id in store_id_list}\n",
    "    historical_data['clean_store_primary_category']\n",
    "    try:\n",
    "        for x in store_id_list:\n",
    "            historical_data['clean_store_primary_category'] = historical_data.store_id.apply(store_id_cat.values[0])\n",
    "    except Exception as ee:\n",
    "        raise ee\n",
    "    return historical_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## update types\n",
    "## create target variable\n",
    "def addFeatures_durations(historical_data:pd.DataFrame) -> pd.DataFrame:\n",
    "    try:\n",
    "        historical_data['created_at'] = pd.to_datetime(historical_data['created_at'])\n",
    "        historical_data['actual_delivery_time'] = pd.to_datetime(historical_data['actual_delivery_time'])\n",
    "        ## calculate delivery duration\n",
    "        historical_data['actual_total_delivery_duration'] = (historical_data['actual_delivery_time'] - historical_data['created_at']).dt.total_seconds()\n",
    "        ## estimated time spent outside the store/not on order preparation\n",
    "        historical_data['est_time_non-prep'] = historical_data['estimated_order_place_duration'] + historical_data['estimated_store_to_consumer_driving_duration']\n",
    "        ## estimated time spent in the store/not driving\n",
    "        historical_data['est_time_prep'] = historical_data['actual_total_delivery_duration'] - historical_data['est_time_non-prep']\n",
    "    except Exception as ex:\n",
    "        raise ex\n",
    "    return historical_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## add ratio features\n",
    "def addFeatures_ratios(historical_data:pd.DataFrame) -> pd.DataFrame:\n",
    "    try:\n",
    "        historical_data['busy_to_onshift'] = historical_data['total_busy_dashers'] / historical_data['total_onshift_dashers']\n",
    "        historical_data['busy_to_outstanding'] = historical_data['total_busy_dashers'] / historical_data['total_outstanding_orders']\n",
    "        historical_data['dasher_total_to_outstanding'] = historical_data['total_outstanding_orders'] / historical_data['total_busy_dashers']\n",
    "        # replace infinite values with NaN to be dropped in the final cleaning step\n",
    "        historical_data['busy_to_onshift'].replace([np.inf,-np.inf],np.nan,inplace=True)\n",
    "        historical_data['busy_to_outstanding'].replace([np.inf,-np.inf],np.nan,inplace=True)\n",
    "        historical_data['dasher_total_to_outstanding'].replace([np.inf,-np.inf],np.nan,inplace=True)\n",
    "    except Exception as ex:\n",
    "        raise ex\n",
    "    return historical_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## add dummies for categories\n",
    "def addFeatures_dummies(historical_data:pd.DataFrame,dummy_column:str) -> pd.DataFrame:\n",
    "    try:\n",
    "        dumm = pd.get_dummies(historical_data[dummy_column],prefix=str(dummy_column + '_'),dtype=float)\n",
    "        # concat dummies\n",
    "        historical_data = pd.concat([historical_data,dumm],axis=1)\n",
    "        historical_data = historical_data.drop(columns=[dummy_column])\n",
    "    except Exception as ex:\n",
    "        raise ex\n",
    "    return historical_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## build the training dataset using the functions above\n",
    "train_df = addFeatures_durations(historical_data=historical_data)\n",
    "train_df = addFeatures_ratios(historical_data=train_df)\n",
    "## add dummy columns for clean_store_primary_category, market_id, order_protocol\n",
    "train_df = addFeatures_dummies(historical_data=train_df,dummy_column='clean_store_primary_category')\n",
    "train_df = addFeatures_dummies(historical_data=train_df,dummy_column='market_id')\n",
    "train_df = addFeatures_dummies(historical_data=train_df,dummy_column='order_protocol')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total_items</th>\n",
       "      <th>subtotal</th>\n",
       "      <th>num_distinct_items</th>\n",
       "      <th>min_item_price</th>\n",
       "      <th>max_item_price</th>\n",
       "      <th>total_onshift_dashers</th>\n",
       "      <th>total_busy_dashers</th>\n",
       "      <th>total_outstanding_orders</th>\n",
       "      <th>estimated_order_place_duration</th>\n",
       "      <th>estimated_store_to_consumer_driving_duration</th>\n",
       "      <th>...</th>\n",
       "      <th>market_id__4.0</th>\n",
       "      <th>market_id__5.0</th>\n",
       "      <th>market_id__6.0</th>\n",
       "      <th>order_protocol__1.0</th>\n",
       "      <th>order_protocol__2.0</th>\n",
       "      <th>order_protocol__3.0</th>\n",
       "      <th>order_protocol__4.0</th>\n",
       "      <th>order_protocol__5.0</th>\n",
       "      <th>order_protocol__6.0</th>\n",
       "      <th>order_protocol__7.0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.0</td>\n",
       "      <td>3441.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>557.0</td>\n",
       "      <td>1239.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>446.0</td>\n",
       "      <td>861.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1900.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1400.0</td>\n",
       "      <td>1400.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>446.0</td>\n",
       "      <td>690.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.0</td>\n",
       "      <td>6900.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>600.0</td>\n",
       "      <td>1800.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>446.0</td>\n",
       "      <td>289.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3900.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>1600.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>446.0</td>\n",
       "      <td>650.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3.0</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>1900.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>446.0</td>\n",
       "      <td>338.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 103 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   total_items  subtotal  num_distinct_items  min_item_price  max_item_price  \\\n",
       "0          4.0    3441.0                 4.0           557.0          1239.0   \n",
       "1          1.0    1900.0                 1.0          1400.0          1400.0   \n",
       "3          6.0    6900.0                 5.0           600.0          1800.0   \n",
       "4          3.0    3900.0                 3.0          1100.0          1600.0   \n",
       "5          3.0    5000.0                 3.0          1500.0          1900.0   \n",
       "\n",
       "   total_onshift_dashers  total_busy_dashers  total_outstanding_orders  \\\n",
       "0                   33.0                14.0                      21.0   \n",
       "1                    1.0                 2.0                       2.0   \n",
       "3                    1.0                 1.0                       2.0   \n",
       "4                    6.0                 6.0                       9.0   \n",
       "5                    2.0                 2.0                       2.0   \n",
       "\n",
       "   estimated_order_place_duration  \\\n",
       "0                           446.0   \n",
       "1                           446.0   \n",
       "3                           446.0   \n",
       "4                           446.0   \n",
       "5                           446.0   \n",
       "\n",
       "   estimated_store_to_consumer_driving_duration  ...  market_id__4.0  \\\n",
       "0                                         861.0  ...             0.0   \n",
       "1                                         690.0  ...             0.0   \n",
       "3                                         289.0  ...             0.0   \n",
       "4                                         650.0  ...             0.0   \n",
       "5                                         338.0  ...             0.0   \n",
       "\n",
       "   market_id__5.0  market_id__6.0  order_protocol__1.0  order_protocol__2.0  \\\n",
       "0             0.0             0.0                  1.0                  0.0   \n",
       "1             0.0             0.0                  0.0                  1.0   \n",
       "3             0.0             0.0                  1.0                  0.0   \n",
       "4             0.0             0.0                  1.0                  0.0   \n",
       "5             0.0             0.0                  1.0                  0.0   \n",
       "\n",
       "   order_protocol__3.0  order_protocol__4.0  order_protocol__5.0  \\\n",
       "0                  0.0                  0.0                  0.0   \n",
       "1                  0.0                  0.0                  0.0   \n",
       "3                  0.0                  0.0                  0.0   \n",
       "4                  0.0                  0.0                  0.0   \n",
       "5                  0.0                  0.0                  0.0   \n",
       "\n",
       "   order_protocol__6.0  order_protocol__7.0  \n",
       "0                  0.0                  0.0  \n",
       "1                  0.0                  0.0  \n",
       "3                  0.0                  0.0  \n",
       "4                  0.0                  0.0  \n",
       "5                  0.0                  0.0  \n",
       "\n",
       "[5 rows x 103 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop the native 'store_primary_category' column and all remaining non-feature columns, then do a headcheck\n",
    "train_df.drop(columns=['store_primary_category','created_at','actual_delivery_time','store_id'],inplace=True)\n",
    "## drop NaN's\n",
    "train_df.dropna(inplace=True)\n",
    "train_df = train_df.astype(\"float32\")\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## identify feature pairs with high correlation\n",
    "def labelMaker_feature_pairs(train_df:pd.DataFrame) -> set:\n",
    "    try:\n",
    "        labeled_pairs = set()\n",
    "        clmn = train_df.columns\n",
    "        for i in range(0,train_df.shape[1]):\n",
    "            for j in range(0,i+1):\n",
    "                labeled_pairs.add((clmn[i],clmn[j]))\n",
    "    except Exception as ex:\n",
    "        raise ex\n",
    "    return labeled_pairs\n",
    "\n",
    "def calcAbsCorr_top_feature_pairs(train_df:pd.DataFrame,n:int) -> list:\n",
    "    try:\n",
    "        abs_corr = train_df.corr(method='pearson').abs().unstack()\n",
    "        drop_labels = labelMaker_feature_pairs(train_df)\n",
    "        abs_corr = abs_corr.drop(labels=drop_labels).sort_values(ascending=False)\n",
    "    except Exception as ex:\n",
    "        raise ex\n",
    "    return abs_corr[0:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "actual_total_delivery_duration                est_time_prep                  0.985830\n",
       "total_onshift_dashers                         total_busy_dashers             0.941300\n",
       "                                              total_outstanding_orders       0.934254\n",
       "total_busy_dashers                            total_outstanding_orders       0.930907\n",
       "estimated_store_to_consumer_driving_duration  est_time_non-prep              0.923236\n",
       "estimated_order_place_duration                order_protocol__1.0            0.897732\n",
       "total_items                                   num_distinct_items             0.757986\n",
       "subtotal                                      num_distinct_items             0.683367\n",
       "busy_to_outstanding                           dasher_total_to_outstanding    0.609299\n",
       "total_items                                   subtotal                       0.557399\n",
       "min_item_price                                max_item_price                 0.540671\n",
       "subtotal                                      max_item_price                 0.507342\n",
       "clean_store_primary_category__fast            order_protocol__4.0            0.490530\n",
       "busy_to_onshift                               busy_to_outstanding            0.487935\n",
       "total_outstanding_orders                      dasher_total_to_outstanding    0.480664\n",
       "num_distinct_items                            min_item_price                 0.446961\n",
       "market_id__2.0                                market_id__4.0                 0.404756\n",
       "total_items                                   min_item_price                 0.389248\n",
       "order_protocol__1.0                           order_protocol__3.0            0.373342\n",
       "estimated_order_place_duration                order_protocol__3.0            0.363913\n",
       "dtype: float64"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calcAbsCorr_top_feature_pairs(train_df=train_df,n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## check for multicollinearity\n",
    "\n",
    "def calcVIF(feature_frame:pd.DataFrame,target_feature:str):\n",
    "    try:\n",
    "        feature_frame = add_constant(feature_frame)\n",
    "        feature_list = feature_frame.drop(columns=[target_feature]).columns.to_list()\n",
    "        vif_frame = pd.DataFrame()\n",
    "        vif_frame['feature'] = feature_list\n",
    "        vif_frame['VIF'] = [variance_inflation_factor(feature_frame[feature_list].values,i) for i in range(len(feature_list))]\n",
    "    except Exception as ex:\n",
    "        raise ex\n",
    "    return vif_frame.sort_values(by=['VIF'],ascending=False).reset_index(drop=True)\n",
    "\n",
    "def removeFeature_VIF(feature_frame:pd.DataFrame,target_feature:str,vif_limit:int):\n",
    "    vif_table = calcVIF(feature_frame=feature_frame,target_feature=target_feature)\n",
    "    try:\n",
    "        ## drop nan columns from feature set, nan VIF values from base VIF table\n",
    "        vif_table['VIF'].replace(to_replace=[np.inf,-np.inf],value=np.nan,inplace=True)\n",
    "        nan_list = vif_table[vif_table['VIF'].isnull()]['feature'].to_list()\n",
    "        feature_frame = feature_frame.drop(columns=nan_list)\n",
    "        vif_table = vif_table.set_index(vif_table.feature)\n",
    "        vif_table = vif_table.drop(labels=nan_list).reset_index(drop=True)\n",
    "        ## start\n",
    "        multicollinearity = True if vif_table['VIF'].values.tolist()[1] > vif_limit else False\n",
    "        while multicollinearity:\n",
    "            lead_vif_feature = vif_table['feature'].values.tolist()[1]\n",
    "            feature_frame = feature_frame.drop(columns=lead_vif_feature)\n",
    "            vif_table = calcVIF(feature_frame=feature_frame,target_feature=target_feature)\n",
    "            multicollinearity = True if vif_table['VIF'].values.tolist()[1] > vif_limit else False\n",
    "    except Exception as ex:\n",
    "        raise ex\n",
    "    return feature_frame,vif_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lukew\\anaconda3\\lib\\site-packages\\statsmodels\\stats\\outliers_influence.py:195: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  vif = 1. / (1. - r_squared_i)\n",
      "c:\\Users\\lukew\\anaconda3\\lib\\site-packages\\statsmodels\\stats\\outliers_influence.py:195: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  vif = 1. / (1. - r_squared_i)\n",
      "c:\\Users\\lukew\\anaconda3\\lib\\site-packages\\statsmodels\\stats\\outliers_influence.py:195: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  vif = 1. / (1. - r_squared_i)\n",
      "c:\\Users\\lukew\\anaconda3\\lib\\site-packages\\statsmodels\\regression\\linear_model.py:1736: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return 1 - self.ssr/self.centered_tss\n"
     ]
    }
   ],
   "source": [
    "trim_train_df,vif_features = removeFeature_VIF(feature_frame=train_df,target_feature='actual_total_delivery_duration',vif_limit=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "## feature selection/dimensionality reduction with PCA\n",
    "##TODO #2 make a class for PCA\n",
    "##TODO #3 retain row numbers in scaler --DONE\n",
    "\n",
    "def build_train_test(dataset:pd.DataFrame,target_feature:str,test_size:int):\n",
    "    try:\n",
    "        X = dataset.drop(columns=target_feature)\n",
    "        y = dataset[target_feature]\n",
    "        x_train,x_test,y_train,y_test = train_test_split(X,y,test_size=test_size,random_state=42)\n",
    "        x_train,x_test = pd.DataFrame(x_train),pd.DataFrame(x_test)\n",
    "    except Exception as ex:\n",
    "        raise ex\n",
    "    return x_train,x_test,y_train,y_test\n",
    "\n",
    "def apply_standard_scaler(dataset):\n",
    "#applies the standard scaler to a single dataset\n",
    "    scaler = StandardScaler()\n",
    "    try:\n",
    "        if type(dataset) == pd.DataFrame:\n",
    "            scaler.fit(dataset.values)\n",
    "            dataset_scaled = scaler.transform(dataset.values)\n",
    "            dataset_scaled = pd.DataFrame(dataset_scaled,index=dataset.index,columns=dataset.columns)\n",
    "        else:\n",
    "            scaler.fit(dataset)\n",
    "            dataset_scaled = scaler.transform(dataset)\n",
    "            test_stdScale = scaler.transform(dataset)\n",
    "    except Exception as ex:\n",
    "        raise ex\n",
    "    return dataset_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PCAcalc_expl_var(x_train,target_expl_variance:int):\n",
    "## returns the number of components required to explain the target_expl_variance\n",
    "# and a feature to component ratio to support automation of further dimensionality reduction\n",
    "    pca = PCA()\n",
    "    x_train_scaled = apply_standard_scaler(x_train)\n",
    "    try:\n",
    "        pca = pca.fit(x_train_scaled)\n",
    "        components = np.cumsum(pca.explained_variance_ratio_)\n",
    "        components = [i for i in components if i <= target_expl_variance]\n",
    "        feature_to_component_ratio = len(components)/x_train.shape[1]\n",
    "    except Exception as ex:\n",
    "        raise ex\n",
    "    return components,feature_to_component_ratio\n",
    "\n",
    "def PCAcalc_dim_reduction(x_train,x_test,desired_components:int):\n",
    "## you know, dimensionality reduction\n",
    "    pca = PCA(n_components=desired_components)\n",
    "    x_train_scaled,x_test_scaled = apply_standard_scaler(x_train),apply_standard_scaler(x_test)\n",
    "    try:\n",
    "        pca = pca.fit(x_train_scaled)\n",
    "        x_train_pca,x_test_pca = pca.transform(x_train_scaled),pca.transform(x_test_scaled)\n",
    "    except Exception as ex:\n",
    "        raise ex\n",
    "    return x_train_pca,x_test_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA() \n",
    "pca.fit(x_train_stdScale)\n",
    "x_train_pca,x_test_pca = pca.transform(x_train_stdScale),pca.transform(x_test_stdScale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7,\n",
       " 95,\n",
       " 0,\n",
       " 4,\n",
       " 9,\n",
       " 13,\n",
       " 92,\n",
       " 99,\n",
       " 98,\n",
       " 96,\n",
       " 89,\n",
       " 91,\n",
       " 100,\n",
       " 66,\n",
       " 62,\n",
       " 19,\n",
       " 39,\n",
       " 28,\n",
       " 35,\n",
       " 84,\n",
       " 54,\n",
       " 84,\n",
       " 73,\n",
       " 55,\n",
       " 94,\n",
       " 22,\n",
       " 73,\n",
       " 21,\n",
       " 30,\n",
       " 75,\n",
       " 49,\n",
       " 22,\n",
       " 77,\n",
       " 17,\n",
       " 27,\n",
       " 50,\n",
       " 81,\n",
       " 40,\n",
       " 87,\n",
       " 81,\n",
       " 87,\n",
       " 69,\n",
       " 45,\n",
       " 31,\n",
       " 38,\n",
       " 58,\n",
       " 44,\n",
       " 29,\n",
       " 70,\n",
       " 67,\n",
       " 70,\n",
       " 86,\n",
       " 26,\n",
       " 41,\n",
       " 41,\n",
       " 59,\n",
       " 15,\n",
       " 20,\n",
       " 47,\n",
       " 78,\n",
       " 53,\n",
       " 64,\n",
       " 80,\n",
       " 76,\n",
       " 42,\n",
       " 72,\n",
       " 23,\n",
       " 34,\n",
       " 16,\n",
       " 16,\n",
       " 48,\n",
       " 85,\n",
       " 32,\n",
       " 94,\n",
       " 82,\n",
       " 55,\n",
       " 101,\n",
       " 101,\n",
       " 91,\n",
       " 100,\n",
       " 93,\n",
       " 51,\n",
       " 99,\n",
       " 3,\n",
       " 96,\n",
       " 12,\n",
       " 92,\n",
       " 98,\n",
       " 3,\n",
       " 13,\n",
       " 0,\n",
       " 2,\n",
       " 8,\n",
       " 5,\n",
       " 6,\n",
       " 90,\n",
       " 19,\n",
       " 90,\n",
       " 52,\n",
       " 10,\n",
       " 36,\n",
       " 18]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_comp = pca.components_.shape[0]\n",
    "important_feature = [np.abs(pca.components_[i]).argmax() for i in range(n_comp)]\n",
    "important_feature\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "96d1af5f869624c072d9e03e4f7fcffed44526ca720f1a36a76cd678c41e0e80"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
